# Phase 1 -- Job Status -- Confirm strategy

## Goals

* Build some working code
* Confirm my planned approach isn't painful to code

## Workspace

I did a [side experiment on workspaces](https://github.com/jmjf/go-workspace-experiment) to understand how they work. I plan to have several executables in this bounded context and want to share common code. So, I need to define my modules.

In the same experiment, I sorted out a basic approach for using clean/DDD/hexagonal/whatever architecture, which I'll be using here.

Here's the logical entities I know about:

* `jobStatus` -- includes references to `application` and `job`
* `application`
* `job`
* `slo` -- includes references to `application` and `sloToJob`
* `sloToJob` -- includes references to `slo` and `job`
* `sloPerformance` -- summary of meet/miss data for each SLO period

To resolve the circular reference between `slo` and `sloToJob`, I'll probably make `slo` an aggregate that includes many `sloToJob` in memory. In the database, it's two different tables. I'll also `job` to have `sloToJob`, so I think I'll have something like this:

```golang
// naming pattern TBD, but this way is good for now to draw distinctions

type SloEntity struct {}
type JobEntity struct {}
type SloJobEntity struct {} // carries ids, not jobs or SLOs
type ApplicationEntity struct {}
type JobStatusEntity struct {}
type SloPerformanceEntity struct {}

type SloAggregate struct{
   Slo SloEntity
   SloJobs []SloJobEntity // we need the relationship entity because it has expected start/end times
   // SLO performance could get large so deal with it in slices that have the range of time of interest
}

type JobAggregate struct {
   Job JobEntity
   JobSlo []SloJobEntity // the relationship can go either way
}

type ApplicationAggregate struct {
   Application ApplicationEntity
   ApplicationSlos []SloAggregate // simple relationship with no extra attributes
}
```

I reserve the right to change my mind as I move forward and better understand how to manage the data.

For phase 1, I'm concerned with `JobStatus`. Data from HTTP includes ids for `job` and `application`. I may build an aggregate that includes their details later in phase 1.

So, let's expect we'll have modules for `JobStatus`, `Slo`, `Job` and `Application` at a minimum. For phase 1 I'll focus on `JobStatus` and build the rest out when I'm ready for them.

It's time to create the workspace.

```bash
# Cleanup from earlier testing
rm go.mod
rm go.sum
rm testdb.go # it's in commit history when I want to see it

# Create the workspace
go work init
mkdir jobStatus && cd jobStatus && go mod init jobStatus
cd ..
```

I'll sort out how to manage the pg connection later. I may try to build a shared connection object. TBD.

## Job status basics

`domain.go` defines the domain types, constants, and common interfaces

* Basic types
* Constants -- all constants for the module begin with `JobStatus_` and end with an all-caps name (ex: `JobStart`)
* Structs -- `JobStatus` and `JobStatusDto`; latter has JSON tags and uses short field names
* Interfaces -- `Repo` and `JobStatusUC`

Build a repo using `database/sql` in `dbSqlRepo.go`. Build a repo using a slice in memory in `memoryRepo.go`. When I start the application, I can create the repo I prefer. Use cases will use the interface, so won't care which implementation I'm using.

For now, I'm leaving `go.work` out of `.gitignore`. Also, I'm changing how I tag commits in markdown files to avoid complaints about full-line emphasis.

**COMMIT:** FEAT: add repos for database/sql and memory; define key data structures; establish workspace

## Summary so far

What's going on and why am I breaking things up this way?

Clean/DDD/etc. architecture models recommend separating implementation details from business logic. That leads to patterns like:

* **Entities**, **aggregates** and similar data structures that include core business logic for managing the data they hold.
  * I haven't build the intelligence on `JobStatus` yet, but it's coming.
  * Aggregates may be composed of entities. They may carry domain event information.
* **Use cases**, services or similar that perform business processes on the entities/aggregates to meet business requirements.

These two types of objects make up the business domain space. Arguably, a non-technical user could read their names and possibly their code and understand them. The domain space may include **domain events** used to notify the system of actions. For example, in `go-slo`, we might have a domain event for "job status created" or "SLO missed" that can be published to subscribers to trigger actions.

* Repos, controllers and other **adapters** that manage the interface between use cases (process) and the outside world (database, HTTP, message bus, logging, etc.)
  * Logging happens in the adapters, not in use cases.
  * Adapters are responsible for translating data from raw, external formats to a form the business domain code recognizes.
* **Infrastructure** represents the outside world and includes database drivers, HTTP servers, etc., that the adapters use.

Separating business logic from external infrastructure lets us change infrastructure by building new adapters. Today we're developing on SQLite. Tomorrow we decide to switch to Postgres or MySQL as we approach release. Next year, our SaaS platform is the new hot thing so we're running on a cloud database or need to support different databases due to different customers' demands. The business logic is decoupled from the infrastructure, allowing them to change independently.

This separation is made possible by defined interfaces. So, look at the `Repo` in `domain.go`. We can provide anything that satisfies that interface to our use cases and they won't care about the physical implementation details. Look at `dbSqlRepo.go` and `memoryRepo.go`. They're completely different ways to store and manage data, but I can give either to a use case as a `Repo` and the use case will work with no changes. And if I decided to replace `database/sql` with direct `pgx` or `gorm` or MySQL or MongoDB, as long as the adapter is a `Repo`, the domain code doesn't care.

Return patterns are part of the interface. Adapters must have a consistent definition of an error. If "not found" returns an error in `dbSqlRepo`, it needs to return an error in other repos or `dbSqlRepo` needs to convert the error to an empty result on return.

## Create tables and write a program to perform simple tests

I noticed I forgot to include the job status timestamp, which tells us when the job reached the status. I've added that in the `dbSqlRepo`. It isn't an issue in `memoryRepo` because it does no data mapping (yet).

The `JobStatus` table has the following DDL. Postgres defines the index as a primary key, which means the combination of columns must be unique. For our testing purposes, that's fine, but I'll need to think about indexing later. (Premature indexing is a root of many evils and much wasted space and server CPU and I/O cycles.)

```sql
CREATE TABLE "public"."JobStatus" (
    "ApplicationId" character varying(200) NOT NULL,
    "JobId" character varying(200) NOT NULL,
    "JobStatusCode" character varying(10) NOT NULL,
    "JobStatusTimestamp" timestamptz NOT NULL,
    "BusinessDate" date NOT NULL,
    "RunId" character varying(50),
    "HostId" character varying(150),
    CONSTRAINT "JobStatus_Primary" PRIMARY KEY ("JobId", "BusinessDate", "JobStatusTimestamp", "ApplicationId")
) WITH (oids = false);
```

I'm testing directly with the repos, so for this test, I'm making the `Repo`'s `add` method exported by changing it to `Add`.

I built `./cmd/testRepo/testRepo.go` to test both repos. Both repos using the same test function, which accepts a repo to test. Think of the test function as a use case using the repo. It doesn't care which repo it uses.

Important things I learned:

* MySQL uses `?` placeholders in parameterized queries, but Postgres uses `$1`, `$2`, etc. So I renamed `dbSqlRepo` to `dbSqlPgRepo` because the query strings in it are Postgres-specific.
* Either `database/sql` or `pgx` is converting table and column names to all lower case unless they're in double-quotes. Postgres is set to run case sensitive, so I added double-quotes in all the query strings.

After some tweaking, both `testRepo` tests `dbSqlPgRepo` and `memoryRepo` with no problems. When testing, ensure the `JobStatus` table is empty to avoid duplicate key errors.

**COMMIT:** TEST: run tests for both repos and confirm implementation agnosticism
